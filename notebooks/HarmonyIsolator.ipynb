{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Harmony Line Isolator\n",
        "\n",
        "## MVP Progress Report: Phase 1 (Steps 0–7)\n",
        "\n",
        "The overall goal of the Harmony Line Isolator MVP is to create a tool that can automatically separate and isolate individual vocal lines from a 3- or 4-part harmony recording — even when the voices are blended together in a single mix — so that each part can be soloed, muted, or repurposed for practice, learning, or creative rearrangement.\n",
        "\n",
        "Phase 1 concentrates on the building blocks needed to reach that goal, using recordings where each vocal part is already available as a separate stem. In this phase, we track the pitch of each part over time, assign musical roles (high, mid, low), and export them as MIDI for quick playback and analysis. This provides a reliable, testable foundation before tackling the more complex step of isolating roles directly from a mixed recording.\n",
        "\n",
        "### Step 0: Download audio samples via index file\n",
        "**What it does:** Reads `audio_file_list.txt` from your GitHub Pages `audio_samples/` folder and downloads each listed `.wav` into a local `audio_samples/` directory in Colab. Skips files already present and prints a summary.\n",
        "**Why it matters:** Keeps your Colab environment in sync with the repo without manual uploads.\n",
        "**Expected output:** A short report showing how many files were downloaded, skipped, or failed, plus a few sample filenames.\n",
        "\n",
        "### Step 1: Environment setup and dataset indexing\n",
        "**What it does:** Installs needed libraries, scans `audio_samples` for `.wav` files, validates the `gw_phraseNN_part.wav` naming, and builds an index of phrases, each with a single mix and multiple stems. Saves `dataset_index.csv`.\n",
        "**Why it matters:** Ensures the dataset is consistent, which avoids hard-to-debug errors later.\n",
        "**Expected output:** A table of phrases with counts and filenames, and a message confirming how many phrases are ready.\n",
        "\n",
        "### Step 2: Load one phrase and validate alignment\n",
        "**What it does:** Loads one phrase’s mix and stems at a common sample rate, prints durations, and trims or pads stems to match the mix length within a small tolerance.\n",
        "**Why it matters:** Even small timing mismatches can throw off pitch tracking and comparisons.\n",
        "**Expected output:** Durations for the mix and each stem, plus a note indicating if any stems were trimmed or padded.\n",
        "\n",
        "### Step 3: Pitch tracking with CREPE (mix and stems)\n",
        "**What it does:** Runs a deep learning pitch detector at 10 ms intervals on the mix and each stem, filtering out low-confidence frames. Reports the percentage of “voiced” frames.\n",
        "**Why it matters:** Produces the pitch contours that power role assignment and MIDI export.\n",
        "**Expected output:** Frame count, analysis duration, voiced coverage for the mix and each stem, and confirmation that pitch arrays were created.\n",
        "\n",
        "### Step 4: Role assignment from stem pitch\n",
        "**What it does:** For each time step, sorts the available stem pitches and assigns roles (high, mid, low) based on relative pitch. Smoothing reduces small jitters. Reports which stem most often supplies each role.\n",
        "**Why it matters:** Converts raw pitch into usable musical roles that match how singers think about parts.\n",
        "**Expected output:** Voiced coverage per role and a mapping like “high → alto” or “low → tenor” most of the time.\n",
        "\n",
        "### Step 5: Export roles as MIDI\n",
        "**What it does:** Segments each role’s pitch contour into notes and writes one MIDI per role, enforcing a minimum note length to reduce blips.\n",
        "**Why it matters:** MIDI is a compact and editable representation you can audition in a DAW or notation tool.\n",
        "**Expected output:** File names of the created MIDIs and note counts for each.\n",
        "\n",
        "### Step 6: Quick audio previews from MIDI\n",
        "**What it does:** Renders simple sine-wave previews from the role MIDIs and saves WAVs so you can listen in Colab without external tools.\n",
        "**Why it matters:** Lets you quickly hear if the extracted lines feel musically correct before attempting vocal-like resynthesis.\n",
        "**Expected output:** Confirmation of generated preview WAVs and inline audio players if running in Colab.\n",
        "\n",
        "### Step 7: MIDI vs stem pitch validation\n",
        "**What it does:** Compares each role’s MIDI to the matching stem by extracting pitch from both on the same timeline. Reports pitch error in cents (mean, median, 90th percentile) and the percentage of frames where both are voiced.\n",
        "**Why it matters:** Gives an objective accuracy check and highlights where settings might need tuning.\n",
        "**Expected output:** A small table with metrics per role. As a rule of thumb, a median error under ~25 cents is very good; 25–50 cents is acceptable; higher suggests further tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "nrQyv53BvH_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0 — Download audio samples via index file\n",
        "\n",
        "This step reads a plain‑text index (`audio_file_list.txt`) hosted in your GitHub Pages `audio_samples/` folder, then downloads each listed `.wav` into a local `audio_samples/` directory in Colab. It skips files that already exist, prints a summary, and fails fast if the index cannot be fetched. Run this once per session before the later steps."
      ],
      "metadata": {
        "id": "4DZGp3dZ7feD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Read audio_file_list.txt from GitHub Pages and download WAVs to audio_samples/\n",
        "\n",
        "!pip -q install requests\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Point to your GitHub Pages location\n",
        "BASE_URL = \"https://harmonyisolator.github.io/mvp/audio_samples/\"\n",
        "INDEX_NAME = \"audio_file_list.txt\"   # the file you uploaded\n",
        "INDEX_URL = urljoin(BASE_URL, INDEX_NAME)\n",
        "\n",
        "LOCAL_DIR = \"audio_samples\"\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "def fetch_index_lines(url, timeout=30):\n",
        "    r = requests.get(url, timeout=timeout)\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"Failed to fetch index: {url} (status {r.status_code})\")\n",
        "    # Keep only non-empty, non-comment lines\n",
        "    lines = [ln.strip() for ln in r.text.splitlines()]\n",
        "    lines = [ln for ln in lines if ln and not ln.startswith(\"#\")]\n",
        "    return lines\n",
        "\n",
        "def download_file(url, out_path, timeout=60, chunk=1<<16):\n",
        "    with requests.get(url, stream=True, timeout=timeout) as r:\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(f\"GET {url} -> {r.status_code}\")\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for blk in r.iter_content(chunk_size=chunk):\n",
        "                if blk:\n",
        "                    f.write(blk)\n",
        "\n",
        "print(f\"[INFO] Fetching index: {INDEX_URL}\")\n",
        "filenames = fetch_index_lines(INDEX_URL)\n",
        "if not filenames:\n",
        "    raise RuntimeError(\"Index is empty. Ensure audio_file_list.txt lists one filename per line.\")\n",
        "\n",
        "print(f\"[INFO] {len(filenames)} files listed in index.\")\n",
        "downloaded = 0\n",
        "skipped = 0\n",
        "errors = []\n",
        "\n",
        "for fname in filenames:\n",
        "    # Basic guard: only accept .wav files in this workflow\n",
        "    if not fname.lower().endswith(\".wav\"):\n",
        "        print(f\"[SKIP] Not a .wav entry: {fname}\")\n",
        "        continue\n",
        "    file_url = urljoin(BASE_URL, fname)\n",
        "    out_path = os.path.join(LOCAL_DIR, os.path.basename(fname))\n",
        "    if os.path.exists(out_path):\n",
        "        skipped += 1\n",
        "        continue\n",
        "    try:\n",
        "        print(f\"[DL] {fname}\")\n",
        "        download_file(file_url, out_path)\n",
        "        downloaded += 1\n",
        "    except Exception as e:\n",
        "        errors.append((fname, str(e)))\n",
        "\n",
        "print(\"\\n[INFO] Download summary\")\n",
        "print(\"   downloaded:\", downloaded)\n",
        "print(\"   skipped (already present):\", skipped)\n",
        "print(\"   errors:\", len(errors))\n",
        "for fname, msg in errors[:10]:\n",
        "    print(\"   -\", fname, \"->\", msg)\n",
        "\n",
        "local_wavs = [f for f in os.listdir(LOCAL_DIR) if f.lower().endswith(\".wav\")]\n",
        "print(f\"[INFO] Local folder now has {len(local_wavs)} .wav files in '{LOCAL_DIR}/'\")\n",
        "\n",
        "# Optional: show a few names\n",
        "print(\"[INFO] Sample files:\", local_wavs[:8])"
      ],
      "metadata": {
        "id": "tqcQB8dT7lVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 — Environment Setup and Dataset Indexing\n",
        "\n",
        "This step prepares the Colab environment for the Harmony Line Isolator MVP.  \n",
        "It installs required libraries, scans the `audio_samples` folder for `.wav` files, validates the naming pattern, and groups files by phrase.  \n",
        "The result is a clean index showing each phrase’s mix file and stems, ready for processing in later steps.\n"
      ],
      "metadata": {
        "id": "WFfj5LZAme5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies used across the MVP\n",
        "!pip -q install librosa crepe pretty_midi soundfile matplotlib pandas\n",
        "print(\"Dependencies installed\")"
      ],
      "metadata": {
        "id": "-EvA51R9mrgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Environment setup and dataset indexing\n",
        "\n",
        "# 1) Imports\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import pandas as pd\n",
        "\n",
        "# 2) Configuration\n",
        "AUDIO_DIR = \"audio_samples\"   # set this to your folder with .wav files\n",
        "INDEX_CSV = \"dataset_index.csv\"\n",
        "\n",
        "# 3) Helpers\n",
        "PATTERN = re.compile(r\"^(gw_phrase\\d+)_([^.]+)\\.wav$\", re.IGNORECASE)\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[INFO] {msg}\")\n",
        "\n",
        "def list_wav_files(folder):\n",
        "    if not os.path.isdir(folder):\n",
        "        raise FileNotFoundError(f\"Folder not found: {folder}\")\n",
        "    return sorted([f for f in os.listdir(folder) if f.lower().endswith(\".wav\")])\n",
        "\n",
        "# 4) Scan folder\n",
        "wav_files = list_wav_files(AUDIO_DIR)\n",
        "log(f\"Found {len(wav_files)} .wav files in '{AUDIO_DIR}'\")\n",
        "\n",
        "# 5) Parse filenames and group by phrase\n",
        "phrase_groups = defaultdict(lambda: {\"mix\": [], \"stems\": []})\n",
        "unmatched = []\n",
        "\n",
        "for fname in wav_files:\n",
        "    m = PATTERN.match(fname)\n",
        "    if not m:\n",
        "        unmatched.append(fname)\n",
        "        continue\n",
        "    phrase = m.group(1).lower()\n",
        "    part = m.group(2).lower()\n",
        "    if part == \"mix\":\n",
        "        phrase_groups[phrase][\"mix\"].append(fname)\n",
        "    else:\n",
        "        phrase_groups[phrase][\"stems\"].append((part, fname))\n",
        "\n",
        "# 6) Basic validations\n",
        "if unmatched:\n",
        "    log(\"Some files did not match 'gw_phraseNN_part.wav':\")\n",
        "    for u in unmatched[:20]:\n",
        "        print(\"   -\", u)\n",
        "    if len(unmatched) > 20:\n",
        "        print(f\"   ... and {len(unmatched) - 20} more\")\n",
        "\n",
        "missing_mix = [p for p, g in phrase_groups.items() if len(g[\"mix\"]) == 0]\n",
        "duplicate_mix = [p for p, g in phrase_groups.items() if len(g[\"mix\"]) > 1]\n",
        "\n",
        "if missing_mix:\n",
        "    log(\"Phrases with no mix file detected:\")\n",
        "    for p in sorted(missing_mix):\n",
        "        print(\"   -\", p)\n",
        "\n",
        "if duplicate_mix:\n",
        "    log(\"Phrases with more than one mix file detected:\")\n",
        "    for p in sorted(duplicate_mix):\n",
        "        print(\"   -\", p, \"->\", phrase_groups[p][\"mix\"])\n",
        "\n",
        "# 7) Build a flat index DataFrame\n",
        "rows = []\n",
        "for phrase, group in sorted(phrase_groups.items()):\n",
        "    mix_files = group[\"mix\"]\n",
        "    stems = group[\"stems\"]\n",
        "    stem_names = [s[0] for s in stems]\n",
        "    rows.append({\n",
        "        \"phrase\": phrase,\n",
        "        \"mix_count\": len(mix_files),\n",
        "        \"mix_file\": mix_files[0] if mix_files else \"\",\n",
        "        \"stem_count\": len(stems),\n",
        "        \"stem_names\": \", \".join(sorted(stem_names)),\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"phrase\", \"mix_count\", \"mix_file\", \"stem_count\", \"stem_names\"]).sort_values(\"phrase\")\n",
        "display(df.head(20))\n",
        "\n",
        "# 8) Save index\n",
        "df.to_csv(INDEX_CSV, index=False)\n",
        "log(f\"Saved dataset index to {INDEX_CSV}\")\n",
        "\n",
        "# 9) Summary prints\n",
        "all_stem_names = []\n",
        "for g in phrase_groups.values():\n",
        "    all_stem_names.extend([s[0] for s in g[\"stems\"]])\n",
        "\n",
        "stem_name_counts = Counter(all_stem_names)\n",
        "log(f\"Detected {len(phrase_groups)} phrases\")\n",
        "log(f\"Distinct stem labels seen: {sorted(stem_name_counts.keys())}\")\n",
        "log(\"Stem label frequency (top 20):\")\n",
        "for name, count in stem_name_counts.most_common(20):\n",
        "    print(f\"   {name}: {count}\")\n",
        "\n",
        "ready_phrases = [p for p, g in phrase_groups.items() if len(g['mix']) == 1 and len(g['stems']) >= 1]\n",
        "log(f\"Phrases ready for processing (1 mix and at least 1 stem): {len(ready_phrases)}\")\n",
        "\n",
        "# 10) Keep data structures for later steps\n",
        "PHRASE_GROUPS = phrase_groups\n",
        "READY_PHRASES = sorted(ready_phrases)\n",
        "\n",
        "print(\"\\nExample ready phrases:\", READY_PHRASES[:10])\n"
      ],
      "metadata": {
        "id": "hGUiapJ-mjNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 — Load one phrase and validate alignment\n",
        "\n",
        "This step loads a single phrase from the dataset to confirm files are readable and aligned. It selects a phrase, loads the mix and stems at native sample rate, prints basic stats, checks duration consistency, and optionally plots quick waveforms. This sanity check prevents chasing downstream errors caused by misaligned audio.\n"
      ],
      "metadata": {
        "id": "c1sC_yYOnKbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load one phrase and validate alignment\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose a phrase to inspect. You can override this with a specific ID like \"gw_phrase03\".\n",
        "phrase_id = READY_PHRASES[0] if READY_PHRASES else None\n",
        "assert phrase_id is not None, \"No ready phrases found. Check Step 1 output.\"\n",
        "\n",
        "print(f\"[INFO] Inspecting phrase: {phrase_id}\")\n",
        "\n",
        "# Gather file paths for this phrase\n",
        "mix_path = os.path.join(AUDIO_DIR, PHRASE_GROUPS[phrase_id][\"mix\"][0])\n",
        "stem_pairs = PHRASE_GROUPS[phrase_id][\"stems\"]  # list of (part, filename)\n",
        "\n",
        "# Load mix at native sample rate, mono for simplicity\n",
        "y_mix, sr = librosa.load(mix_path, sr=None, mono=True)\n",
        "dur_mix = len(y_mix) / sr\n",
        "\n",
        "print(f\"[INFO] Loaded mix: {os.path.basename(mix_path)}\")\n",
        "print(f\"[INFO] Sample rate: {sr} Hz\")\n",
        "print(f\"[INFO] Mix duration: {dur_mix:.3f} s\")\n",
        "print(f\"[INFO] Found {len(stem_pairs)} stems:\", [p for p, _ in stem_pairs])\n",
        "\n",
        "# Load stems and track durations\n",
        "stems_audio = {}\n",
        "durations = {}\n",
        "\n",
        "for part, fname in stem_pairs:\n",
        "    path = os.path.join(AUDIO_DIR, fname)\n",
        "    y, sr_stem = librosa.load(path, sr=sr, mono=True)  # force same sr as mix\n",
        "    stems_audio[part] = y\n",
        "    durations[part] = len(y) / sr\n",
        "    if sr_stem != sr:\n",
        "        print(f\"[WARN] {fname} sample rate was {sr_stem} and was resampled to {sr}\")\n",
        "\n",
        "# Alignment check: compare lengths\n",
        "print(\"\\n[INFO] Duration check (seconds):\")\n",
        "print(f\"   mix: {dur_mix:.3f}\")\n",
        "for part in sorted(stems_audio.keys()):\n",
        "    print(f\"   {part}: {durations[part]:.3f}\")\n",
        "\n",
        "# Simple tolerance for small mismatches due to trims or silence\n",
        "TOL = 0.02  # 20 ms\n",
        "\n",
        "def trim_or_pad_to_match(y, target_len):\n",
        "    if len(y) == target_len:\n",
        "        return y\n",
        "    if len(y) > target_len:\n",
        "        return y[:target_len]\n",
        "    pad = target_len - len(y)\n",
        "    return np.pad(y, (0, pad), mode=\"constant\")\n",
        "\n",
        "target_len = len(y_mix)\n",
        "fixed = {}\n",
        "\n",
        "needs_fix = []\n",
        "for part, y in stems_audio.items():\n",
        "    if abs(len(y) - target_len) / sr > TOL:\n",
        "        needs_fix.append((part, (len(y) - target_len) / sr))\n",
        "\n",
        "if needs_fix:\n",
        "    print(\"\\n[WARN] Some stems differ from mix length by more than 20 ms:\")\n",
        "    for part, diff_s in needs_fix:\n",
        "        print(f\"   {part}: {diff_s:+.3f} s difference\")\n",
        "    print(\"[INFO] Trimming or padding stems to match mix length for this session view.\")\n",
        "\n",
        "# Create aligned copies for analysis\n",
        "for part, y in stems_audio.items():\n",
        "    fixed[part] = trim_or_pad_to_match(y, target_len)\n",
        "\n",
        "# Optional quick plots\n",
        "PLOT = True\n",
        "if PLOT:\n",
        "    # Plot mix and up to three stems to keep it readable\n",
        "    to_plot = [\"mix\"] + sorted(list(fixed.keys()))[:3]\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = plt.gca()\n",
        "    t = np.arange(len(y_mix)) / sr\n",
        "    ax.plot(t, y_mix, linewidth=0.8, label=\"mix\")\n",
        "    for part in to_plot[1:]:\n",
        "        ax.plot(t, fixed[part], linewidth=0.6, alpha=0.8, label=part)\n",
        "    ax.set_title(f\"Waveforms for {phrase_id} (mix and first stems)\")\n",
        "    ax.set_xlabel(\"Time (s)\")\n",
        "    ax.set_ylabel(\"Amplitude\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n[INFO] Step 2 complete.\")\n",
        "print(\"[INFO] Variables available for next step:\")\n",
        "print(\"   phrase_id      ->\", phrase_id)\n",
        "print(\"   y_mix, sr      -> NumPy array audio and sample rate\")\n",
        "print(\"   stems_audio    -> dict of raw stems by part\")\n",
        "print(\"   fixed          -> dict of length-aligned stems by part\")\n"
      ],
      "metadata": {
        "id": "vpS8enzXnMVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 — Pitch tracking with CREPE on mix and stems\n",
        "\n",
        "This step estimates fundamental frequency (f0) for the selected phrase using CREPE. It runs on the mix and on each stem, aligns frame lengths, applies a confidence mask, and reports voiced coverage. Quick plots help verify that tracks look sensible before role assignment. Results are saved for the next step.\n"
      ],
      "metadata": {
        "id": "ME9zKiWAodke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Pitch tracking with CREPE on mix and stems\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import crepe\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use variables from Step 2: phrase_id, y_mix, sr, fixed (aligned stems)\n",
        "print(f\"[INFO] Pitch tracking for {phrase_id}\")\n",
        "\n",
        "TARGET_SR = 16000\n",
        "STEP_MS = 10\n",
        "CONF_THR = 0.5\n",
        "USE_VITERBI = True\n",
        "\n",
        "def estimate_f0_crepe(y, sr, step_ms=10, conf_thr=0.5, viterbi=True):\n",
        "    \"\"\"Resample to 16 kHz, run CREPE, and mask low-confidence frames.\"\"\"\n",
        "    y16 = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
        "    time, freq, conf, _ = crepe.predict(\n",
        "        y16, TARGET_SR, step_size=step_ms, viterbi=viterbi, model_capacity=\"full\"\n",
        "    )\n",
        "    freq = freq.astype(np.float32)\n",
        "    conf = conf.astype(np.float32)\n",
        "    freq_masked = freq.copy()\n",
        "    freq_masked[conf < conf_thr] = np.nan\n",
        "    return time, freq_masked, conf\n",
        "\n",
        "# 1) Mix f0\n",
        "t_mix, f0_mix, c_mix = estimate_f0_crepe(y_mix, sr, step_ms=STEP_MS, conf_thr=CONF_THR, viterbi=USE_VITERBI)\n",
        "T_len = len(f0_mix)\n",
        "\n",
        "# 2) Stem f0s\n",
        "f0_stems = {}\n",
        "c_stems = {}\n",
        "for part in sorted(fixed.keys()):\n",
        "    t_s, f0_s, c_s = estimate_f0_crepe(fixed[part], sr, step_ms=STEP_MS, conf_thr=CONF_THR, viterbi=USE_VITERBI)\n",
        "    # Keep lengths consistent by trimming to the shortest\n",
        "    min_len = min(T_len, len(f0_s))\n",
        "    if min_len != T_len:\n",
        "        T_len = min_len\n",
        "    f0_stems[part] = f0_s\n",
        "    c_stems[part] = c_s\n",
        "\n",
        "# 3) Align all to common length\n",
        "f0_mix = f0_mix[:T_len]\n",
        "c_mix = c_mix[:T_len]\n",
        "t_crepe = t_mix[:T_len]  # CREPE time base at 16 kHz\n",
        "\n",
        "for part in list(f0_stems.keys()):\n",
        "    f0_stems[part] = f0_stems[part][:T_len]\n",
        "    c_stems[part] = c_stems[part][:T_len]\n",
        "\n",
        "# 4) Coverage stats\n",
        "def voiced_pct(f0):\n",
        "    total = len(f0)\n",
        "    voiced = np.sum(~np.isnan(f0))\n",
        "    return 100.0 * voiced / max(total, 1)\n",
        "\n",
        "print(f\"[INFO] Frames (CREPE): {T_len}, step {STEP_MS} ms, duration ~ {T_len * STEP_MS / 1000:.2f} s\")\n",
        "print(f\"[INFO] Mix voiced coverage: {voiced_pct(f0_mix):.1f}%\")\n",
        "for part in sorted(f0_stems.keys()):\n",
        "    print(f\"   {part:>8} voiced coverage: {voiced_pct(f0_stems[part]):.1f}%\")\n",
        "\n",
        "# 5) Quick visualization\n",
        "PLOT = True\n",
        "if PLOT:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Plot mix f0\n",
        "    plt.plot(t_crepe, f0_mix, linewidth=1.2, label=\"mix f0 (Hz)\")\n",
        "    # Plot up to three stems for readability\n",
        "    for i, part in enumerate(sorted(f0_stems.keys())[:3]):\n",
        "        plt.plot(t_crepe, f0_stems[part], linewidth=0.9, alpha=0.9, label=f\"{part} f0 (Hz)\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Frequency (Hz)\")\n",
        "    plt.title(f\"CREPE f0 for {phrase_id} (mix and stems)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 6) Persist results for the next step (role assignment)\n",
        "F0_TIME = t_crepe\n",
        "F0_MIX = f0_mix\n",
        "CONF_MIX = c_mix\n",
        "F0_STEMS = f0_stems\n",
        "CONF_STEMS = c_stems\n",
        "\n",
        "print(\"\\n[INFO] Step 3 complete.\")\n",
        "print(\"[INFO] Variables available for next step:\")\n",
        "print(\"   F0_TIME    -> time base from CREPE (seconds)\")\n",
        "print(\"   F0_MIX     -> f0 track for mix (Hz, NaN for unvoiced)\")\n",
        "print(\"   F0_STEMS   -> dict of f0 tracks per part\")\n",
        "print(\"   CONF_MIX   -> CREPE confidence for mix\")\n",
        "print(\"   CONF_STEMS -> dict of CREPE confidence per part\")\n"
      ],
      "metadata": {
        "id": "-vEvefBQogDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 — Role assignment from stem f0 tracks\n",
        "\n",
        "This step assigns musical roles per frame using the stem f0 tracks. It ranks available pitches and labels them as low, mid, or high, depending on how many parts are present. It applies light smoothing, reports coverage, and plots the result. Outputs feed the MIDI export in the next step.\n"
      ],
      "metadata": {
        "id": "sf-Tofl4q3sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Role assignment from stem f0 tracks (adaptive to 2, 3, or 4 parts)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Uses from Step 3:\n",
        "#   phrase_id, F0_TIME, F0_STEMS (dict part->f0 ndarray), F0_MIX (optional), CONF_STEMS\n",
        "print(f\"[INFO] Role assignment for {phrase_id}\")\n",
        "\n",
        "# 1) Pack stems into a matrix [P, T]\n",
        "part_names = sorted(F0_STEMS.keys())\n",
        "assert len(part_names) >= 2, \"Need at least two stems for role assignment.\"\n",
        "\n",
        "F = np.vstack([F0_STEMS[p] for p in part_names])  # shape [P, T], Hz with NaN for unvoiced\n",
        "T = F.shape[1]\n",
        "\n",
        "def median_smooth_vec(x, k=7):\n",
        "    s = pd.Series(x)\n",
        "    s = s.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    return s.rolling(k, center=True, min_periods=1).median().values\n",
        "\n",
        "# 2) Optional light smoothing on each stem f0 to reduce jitter\n",
        "F_smooth = F.copy()\n",
        "for i in range(F.shape[0]):\n",
        "    mask = ~np.isnan(F[i])\n",
        "    if mask.any():\n",
        "        F_smooth[i, mask] = median_smooth_vec(F[i, mask], k=7)\n",
        "\n",
        "# 3) Assign roles per frame by sorting available pitches\n",
        "roles_avail = [\"low\", \"mid\", \"high\"]  # we will drop \"mid\" when only 2 parts exist\n",
        "\n",
        "role_f0 = {r: np.full(T, np.nan, dtype=float) for r in roles_avail}\n",
        "role_src = {r: np.full(T, -1, dtype=int) for r in roles_avail}  # index of source part per frame\n",
        "\n",
        "for t in range(T):\n",
        "    vals = [(i, F_smooth[i, t]) for i in range(F_smooth.shape[0]) if not np.isnan(F_smooth[i, t])]\n",
        "    if not vals:\n",
        "        continue\n",
        "    vals.sort(key=lambda x: x[1])  # by frequency ascending\n",
        "    if len(vals) >= 3:\n",
        "        mapping = [(\"low\", 0), (\"mid\", 1), (\"high\", -1)]\n",
        "    elif len(vals) == 2:\n",
        "        mapping = [(\"low\", 0), (\"high\", -1)]  # no \"mid\"\n",
        "    else:  # single pitch present\n",
        "        mapping = [(\"high\", -1)]\n",
        "    for role, pos in mapping:\n",
        "        idx = vals[pos][0]\n",
        "        role_src[role][t] = idx\n",
        "        role_f0[role][t] = vals[pos][1]\n",
        "\n",
        "# 4) Post smoothing on role f0 to improve continuity\n",
        "for r in list(role_f0.keys()):\n",
        "    f = role_f0[r]\n",
        "    if np.all(np.isnan(f)):\n",
        "        continue\n",
        "    mask = ~np.isnan(f)\n",
        "    role_f0[r][mask] = median_smooth_vec(f[mask], k=11)\n",
        "\n",
        "# 5) Coverage and basic stats\n",
        "def voiced_pct(x):\n",
        "    return 100.0 * np.sum(~np.isnan(x)) / max(len(x), 1)\n",
        "\n",
        "print(\"[INFO] Role voiced coverage:\")\n",
        "for r in roles_avail:\n",
        "    if np.all(np.isnan(role_f0[r])):\n",
        "        continue\n",
        "    print(f\"   {r:>4}: {voiced_pct(role_f0[r]):.1f}%\")\n",
        "\n",
        "# Map roles to most common source part for a quick sanity check\n",
        "for r in roles_avail:\n",
        "    src = role_src[r]\n",
        "    src = src[src >= 0]\n",
        "    if len(src) == 0:\n",
        "        continue\n",
        "    counts = pd.Series(src).value_counts()\n",
        "    top_idx = counts.index[0]\n",
        "    print(f\"[INFO] Role '{r}' most often comes from stem: {part_names[top_idx]}  ({int(100*counts.iloc[0]/len(src))}% of voiced frames)\")\n",
        "\n",
        "# 6) Plot roles vs stems for a quick visual check\n",
        "PLOT = True\n",
        "if PLOT:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # plot stems faint\n",
        "    for i, p in enumerate(part_names):\n",
        "        plt.plot(F0_TIME, F_smooth[i], linewidth=0.8, alpha=0.5, label=f\"{p} stem f0\")\n",
        "    # plot roles thicker\n",
        "    color_order = {\"low\": None, \"mid\": None, \"high\": None}  # let matplotlib pick defaults\n",
        "    for r in roles_avail:\n",
        "        if np.all(np.isnan(role_f0[r])):\n",
        "            continue\n",
        "        plt.plot(F0_TIME, role_f0[r], linewidth=2.0, label=f\"{r} role f0\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Frequency (Hz)\")\n",
        "    plt.title(f\"Role assignment for {phrase_id}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 7) Persist for next step (MIDI export)\n",
        "ROLE_F0 = role_f0       # dict role -> f0 array\n",
        "ROLE_SRC = role_src     # dict role -> source index over time\n",
        "ROLE_PART_NAMES = part_names\n",
        "\n",
        "print(\"\\n[INFO] Step 4 complete.\")\n",
        "print(\"[INFO] Variables available for next step:\")\n",
        "print(\"   ROLE_F0          -> dict of role f0 tracks: 'low', 'mid', 'high' (some may be NaN)\")\n",
        "print(\"   ROLE_SRC         -> dict of per-frame source part indices for each role\")\n",
        "print(\"   ROLE_PART_NAMES  -> list mapping source indices to part names\")\n"
      ],
      "metadata": {
        "id": "KgfHA-iqq5eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 — Export roles as MIDI for quick listening and verification\n",
        "\n",
        "This step converts role f0 tracks into simple note sequences and writes a MIDI file per role. MIDI makes it easy to check pitch and phrasing in a DAW or notation app, and is a lightweight way to evaluate separation quality before attempting audio resynthesis. Files are saved alongside your audio.\n"
      ],
      "metadata": {
        "id": "akdh_YXBrW6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Export roles as MIDI\n",
        "\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "\n",
        "# Replace the deprecated fillna(method=...) usage from Step 4, if you re-use it later\n",
        "def median_smooth_vec(x, k=7):\n",
        "    import pandas as pd\n",
        "    s = pd.Series(x)\n",
        "    s = s.ffill().bfill()\n",
        "    return s.rolling(k, center=True, min_periods=1).median().values\n",
        "\n",
        "def f0_to_midi_notes(times_s, f0_hz, min_note_ms=80):\n",
        "    \"\"\"\n",
        "    Convert an f0 curve to monophonic notes with simple segmentation.\n",
        "    Drops NaNs. Merges short blips. Returns a list of (start, end, midi_pitch).\n",
        "    \"\"\"\n",
        "    if len(times_s) == 0 or len(f0_hz) == 0:\n",
        "        return []\n",
        "    assert len(times_s) == len(f0_hz)\n",
        "    dt = np.median(np.diff(times_s)) if len(times_s) > 1 else 0.01\n",
        "    min_frames = max(1, int((min_note_ms / 1000.0) / dt))\n",
        "\n",
        "    notes = []\n",
        "    cur_pitch = None\n",
        "    cur_start = None\n",
        "\n",
        "    for i, hz in enumerate(f0_hz):\n",
        "        if np.isnan(hz):\n",
        "            # close any open note\n",
        "            if cur_pitch is not None:\n",
        "                # end at current frame time\n",
        "                end_t = times_s[i]\n",
        "                # enforce minimum length\n",
        "                if int((end_t - cur_start) / dt) >= min_frames:\n",
        "                    notes.append((cur_start, end_t, cur_pitch))\n",
        "                cur_pitch, cur_start = None, None\n",
        "            continue\n",
        "\n",
        "        pitch = int(round(pretty_midi.hz_to_note_number(hz)))\n",
        "\n",
        "        if cur_pitch is None:\n",
        "            cur_pitch, cur_start = pitch, times_s[i]\n",
        "        elif pitch != cur_pitch:\n",
        "            # close previous\n",
        "            end_t = times_s[i]\n",
        "            if int((end_t - cur_start) / dt) >= min_frames:\n",
        "                notes.append((cur_start, end_t, cur_pitch))\n",
        "            # start new\n",
        "            cur_pitch, cur_start = pitch, times_s[i]\n",
        "\n",
        "    # close tail\n",
        "    if cur_pitch is not None:\n",
        "        end_t = times_s[-1] + dt\n",
        "        if int((end_t - cur_start) / dt) >= min_frames:\n",
        "            notes.append((cur_start, end_t, cur_pitch))\n",
        "\n",
        "    return notes\n",
        "\n",
        "def write_role_midi(role_name, times_s, f0_hz, out_path, program=54):\n",
        "    \"\"\"\n",
        "    Writes a single-track MIDI file for a role.\n",
        "    Default program 54 is a choir-like synth in General MIDI.\n",
        "    \"\"\"\n",
        "    # Guard: skip entirely empty role\n",
        "    if np.all(np.isnan(f0_hz)):\n",
        "        print(f\"[INFO] Role '{role_name}' has no voiced frames. Skipping MIDI.\")\n",
        "        return False\n",
        "\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    inst = pretty_midi.Instrument(program=program, name=role_name)\n",
        "\n",
        "    notes = f0_to_midi_notes(times_s, f0_hz, min_note_ms=80)\n",
        "    for start, end, pitch in notes:\n",
        "        inst.notes.append(pretty_midi.Note(velocity=80, pitch=pitch, start=start, end=end))\n",
        "\n",
        "    if len(inst.notes) == 0:\n",
        "        print(f\"[INFO] Role '{role_name}' produced zero notes after filtering. Skipping.\")\n",
        "        return False\n",
        "\n",
        "    pm.instruments.append(inst)\n",
        "    pm.write(out_path)\n",
        "    print(f\"[OK] Wrote {out_path} with {len(inst.notes)} notes.\")\n",
        "    return True\n",
        "\n",
        "# Export available roles for the current phrase\n",
        "exported = []\n",
        "for role in [\"low\", \"mid\", \"high\"]:\n",
        "    if role not in ROLE_F0:\n",
        "        continue\n",
        "    out_mid = f\"{phrase_id}_{role}.mid\"\n",
        "    ok = write_role_midi(role, F0_TIME, ROLE_F0[role], out_mid, program=54)\n",
        "    if ok:\n",
        "        exported.append(out_mid)\n",
        "\n",
        "print(\"\\n[INFO] Step 5 complete.\")\n",
        "print(\"[INFO] Exported MIDI files:\", exported)\n",
        "print(\"[INFO] You can drag these into a DAW or MuseScore to inspect the lines.\")\n"
      ],
      "metadata": {
        "id": "gyt0Onxera53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6 — Quick audio previews from MIDI (listen in Colab)\n",
        "\n",
        "This step turns each role’s MIDI into a simple synthesized preview so you can hear the separation without any heavy models. It renders sine‑based audio with a tiny fade to avoid clicks, saves WAVs, and embeds players in Colab. This is just for auditing pitch and phrasing before fancier resynthesis.\n"
      ],
      "metadata": {
        "id": "tzlsKqyUrlXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Render simple audio previews from role MIDIs and play them in Colab\n",
        "\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "SR_PREVIEW = 16000  # lightweight sample rate for quick previews\n",
        "\n",
        "def render_sine(f_hz, dur_s, sr=SR_PREVIEW):\n",
        "    n = int(dur_s * sr)\n",
        "    t = np.arange(n) / sr\n",
        "    y = np.sin(2 * np.pi * f_hz * t)\n",
        "    # tiny fade to avoid clicks\n",
        "    fade = int(0.01 * sr)\n",
        "    if fade > 0 and n > 2 * fade:\n",
        "        ramp = np.linspace(0, 1, fade)\n",
        "        y[:fade] *= ramp\n",
        "        y[-fade:] *= ramp[::-1]\n",
        "    return y\n",
        "\n",
        "def midi_to_preview_wav(midi_path, out_wav, sr=SR_PREVIEW):\n",
        "    pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "    dur = pm.get_end_time()\n",
        "    if dur <= 0:\n",
        "        print(f\"[WARN] {midi_path} has zero duration.\")\n",
        "        return None\n",
        "    y = np.zeros(int(np.ceil(dur * sr)), dtype=np.float32)\n",
        "    for inst in pm.instruments:\n",
        "        for note in inst.notes:\n",
        "            f = pretty_midi.note_number_to_hz(note.pitch)\n",
        "            start = int(note.start * sr)\n",
        "            end = int(note.end * sr)\n",
        "            y[start:end] += 0.25 * render_sine(f, (end - start) / sr, sr=sr)\n",
        "    # normalize gently\n",
        "    peak = np.max(np.abs(y)) or 1.0\n",
        "    y = (y / peak).astype(np.float32)\n",
        "    sf.write(out_wav, y, sr)\n",
        "    return out_wav\n",
        "\n",
        "exported = [f\"{phrase_id}_low.mid\", f\"{phrase_id}_mid.mid\", f\"{phrase_id}_high.mid\"]\n",
        "made = []\n",
        "for m in exported:\n",
        "    try:\n",
        "        wav = m.replace(\".mid\", \"_preview.wav\")\n",
        "        path = midi_to_preview_wav(m, wav, sr=SR_PREVIEW)\n",
        "        if path:\n",
        "            made.append(path)\n",
        "            print(f\"[OK] Wrote {path}\")\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "print(\"\\n[INFO] Inline players below (if any previews were created):\")\n",
        "for w in made:\n",
        "    display(Audio(w, rate=SR_PREVIEW))\n"
      ],
      "metadata": {
        "id": "mPLM2ubArnYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7 — MIDI vs stem pitch validation (stats + plot)\n",
        "\n",
        "This cell compares each role’s MIDI to its matching stem audio. It extracts f0 from the stem with CREPE, converts MIDI notes to f0 on a 10 ms grid, and computes pitch error in cents on frames where both are voiced. It reports coverage and error stats and plots the two curves for a quick visual check.\n"
      ],
      "metadata": {
        "id": "XIUMk45Pth6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Validate MIDI vs stem with pitch stats\n",
        "\n",
        "# Requirements: pretty_midi, librosa, crepe, matplotlib already installed\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import pretty_midi\n",
        "import matplotlib.pyplot as plt\n",
        "import crepe\n",
        "\n",
        "# Configuration\n",
        "STEP_MS = 10          # analysis hop, must match your earlier CREPE step\n",
        "CONF_THR = 0.5        # confidence threshold for stem f0\n",
        "TARGET_SR = 16000     # CREPE expects 16 kHz\n",
        "ROLES_TO_CHECK = [\"low\", \"mid\", \"high\"]  # will skip roles without MIDI\n",
        "\n",
        "# Uses these from earlier cells:\n",
        "# - AUDIO_DIR\n",
        "# - phrase_id\n",
        "# - PHRASE_GROUPS\n",
        "# - ROLE_SRC, ROLE_PART_NAMES  (optional, used to auto-map role -> stem)\n",
        "\n",
        "def estimate_f0_crepe(y, sr, step_ms=10, thr=0.5):\n",
        "    y16 = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
        "    times, f0, conf, _ = crepe.predict(\n",
        "        y16, TARGET_SR, step_size=step_ms, viterbi=True, model_capacity=\"full\"\n",
        "    )\n",
        "    f0 = f0.astype(np.float32)\n",
        "    conf = conf.astype(np.float32)\n",
        "    f0[conf < thr] = np.nan\n",
        "    return times, f0, conf\n",
        "\n",
        "def midi_to_f0_series(pm, step_ms=10):\n",
        "    \"\"\"Rasterize MIDI to an f0 series on uniform time steps.\"\"\"\n",
        "    dur = pm.get_end_time()\n",
        "    if dur <= 0:\n",
        "        return np.array([0.0]), np.array([np.nan], dtype=np.float32)\n",
        "    dt = step_ms / 1000.0\n",
        "    times = np.arange(0.0, dur + dt/2, dt)\n",
        "    f0 = np.full_like(times, np.nan, dtype=np.float32)\n",
        "    # Assume monophonic per role\n",
        "    for inst in pm.instruments:\n",
        "        for note in inst.notes:\n",
        "            start_idx = int(np.floor(note.start / dt))\n",
        "            end_idx = int(np.ceil(note.end / dt))\n",
        "            hz = pretty_midi.note_number_to_hz(note.pitch)\n",
        "            f0[start_idx:end_idx] = hz\n",
        "    return times, f0\n",
        "\n",
        "def cents_error(f_ref, f_est):\n",
        "    mask = (~np.isnan(f_ref)) & (~np.isnan(f_est)) & (f_ref > 0) & (f_est > 0)\n",
        "    if not mask.any():\n",
        "        return np.array([]), mask\n",
        "    err = 1200.0 * np.log2(f_est[mask] / f_ref[mask])\n",
        "    return err, mask\n",
        "\n",
        "def most_common_src_index(src_indices):\n",
        "    src = src_indices[src_indices >= 0]\n",
        "    if len(src) == 0:\n",
        "        return None\n",
        "    return int(pd.Series(src).mode().iloc[0])\n",
        "\n",
        "def infer_role_to_stem(phrase_id):\n",
        "    \"\"\"Try to infer the best stem filename for each role using ROLE_SRC majority vote.\"\"\"\n",
        "    role_to_stem = {}\n",
        "    if 'ROLE_SRC' in globals() and 'ROLE_PART_NAMES' in globals():\n",
        "        for role, arr in ROLE_SRC.items():\n",
        "            idx = most_common_src_index(arr)\n",
        "            if idx is None:\n",
        "                continue\n",
        "            part_name = ROLE_PART_NAMES[idx]\n",
        "            # find filename in phrase group\n",
        "            for part, fname in PHRASE_GROUPS[phrase_id][\"stems\"]:\n",
        "                if part == part_name:\n",
        "                    role_to_stem[role] = fname\n",
        "                    break\n",
        "    return role_to_stem\n",
        "\n",
        "# Build role -> stem map\n",
        "role_to_stem = infer_role_to_stem(phrase_id)\n",
        "\n",
        "# Fallback: if not inferred, try name heuristics\n",
        "if not role_to_stem:\n",
        "    heur = {\n",
        "        \"high\": [\"soprano\", \"alto\", \"tenor2\", \"tenor1\", \"tenor\"],\n",
        "        \"mid\":  [\"tenor\", \"tenor1\", \"tenor2\", \"alto\", \"baritone\"],\n",
        "        \"low\":  [\"baritone\", \"bass\", \"tenor\", \"tenor1\"]\n",
        "    }\n",
        "    stems_here = dict(PHRASE_GROUPS[phrase_id][\"stems\"])\n",
        "    for role in ROLES_TO_CHECK:\n",
        "        for cand in heur.get(role, []):\n",
        "            if cand in stems_here:\n",
        "                role_to_stem[role] = stems_here[cand]\n",
        "                break\n",
        "\n",
        "print(\"[INFO] Role → stem mapping to evaluate:\")\n",
        "for r in ROLES_TO_CHECK:\n",
        "    midi_file = f\"{phrase_id}_{r}.mid\"\n",
        "    stem_file = role_to_stem.get(r)\n",
        "    exists_midi = os.path.exists(midi_file)\n",
        "    print(f\"   {r:>4}: MIDI={exists_midi and midi_file or 'missing'}  |  stem={stem_file or 'not mapped'}\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for role in ROLES_TO_CHECK:\n",
        "    midi_path = f\"{phrase_id}_{role}.mid\"\n",
        "    stem_fname = role_to_stem.get(role)\n",
        "    if not (os.path.exists(midi_path) and stem_fname):\n",
        "        continue\n",
        "\n",
        "    # Load stem audio\n",
        "    stem_path = os.path.join(AUDIO_DIR, stem_fname)\n",
        "    y, sr = librosa.load(stem_path, sr=None, mono=True)\n",
        "\n",
        "    # f0 for stem\n",
        "    t_stem, f0_stem, conf = estimate_f0_crepe(y, sr, step_ms=STEP_MS, thr=CONF_THR)\n",
        "\n",
        "    # f0 for MIDI\n",
        "    pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "    t_midi, f0_midi = midi_to_f0_series(pm, step_ms=STEP_MS)\n",
        "\n",
        "    # Align to a common time base\n",
        "    T = min(len(t_stem), len(t_midi))\n",
        "    t = t_stem[:T]\n",
        "    f0_s = f0_stem[:T]\n",
        "    f0_m = f0_midi[:T]\n",
        "\n",
        "    # Metrics\n",
        "    err_cents, mask = cents_error(f0_s, f0_m)\n",
        "    voiced_overlap_pct = 100.0 * np.mean(mask) if len(mask) else 0.0\n",
        "    stats = {}\n",
        "    if err_cents.size:\n",
        "        stats = {\n",
        "            \"mean_abs_cents\": float(np.mean(np.abs(err_cents))),\n",
        "            \"median_abs_cents\": float(np.median(np.abs(err_cents))),\n",
        "            \"p90_abs_cents\": float(np.percentile(np.abs(err_cents), 90)),\n",
        "            \"signed_mean_cents\": float(np.mean(err_cents)),\n",
        "            \"voiced_overlap_pct\": float(voiced_overlap_pct)\n",
        "        }\n",
        "    else:\n",
        "        stats = {\n",
        "            \"mean_abs_cents\": np.nan,\n",
        "            \"median_abs_cents\": np.nan,\n",
        "            \"p90_abs_cents\": np.nan,\n",
        "            \"signed_mean_cents\": np.nan,\n",
        "            \"voiced_overlap_pct\": float(voiced_overlap_pct)\n",
        "        }\n",
        "\n",
        "    results.append({\"phrase\": phrase_id, \"role\": role, \"stem\": stem_fname, **stats})\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(t, f0_s, linewidth=1.2, label=f\"stem f0: {stem_fname}\")\n",
        "    plt.plot(t, f0_m, linewidth=2.0, alpha=0.9, label=f\"MIDI f0: {role}\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Frequency (Hz)\")\n",
        "    plt.title(f\"Pitch comparison: {phrase_id} — {role}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Summary table\n",
        "df_res = pd.DataFrame(results)\n",
        "display(df_res)\n",
        "\n",
        "# Save results\n",
        "out_csv = f\"{phrase_id}_midi_vs_stem_pitch_stats.csv\"\n",
        "df_res.to_csv(out_csv, index=False)\n",
        "print(f\"[OK] Saved stats to {out_csv}\")\n",
        "\n",
        "# Quick read on quality\n",
        "if not df_res.empty:\n",
        "    print(\"\\n[INFO] Rough guide: <25 cents median is very good, 25–50 cents acceptable, >50 cents needs attention.\")\n"
      ],
      "metadata": {
        "id": "Kl089e9Qtjx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}